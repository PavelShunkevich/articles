{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626dc3d4-cc86-42a6-afcd-f8f055ebbd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on device cuda.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import datetime\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split  # Импортируем train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Выбираем устройство для обучения (GPU, если доступен, иначе CPU)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training and evaluating on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f82dee66-bcf9-4dd0-8d0c-fbbbfc5530dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_and_std(loader: DataLoader, dim: list, device=\"cpu\") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Вычисление среднего значения и стандартного отклонения в наборе данных.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader для итерации по набору данных\n",
    "        dim (list): Размерности, по которым будут считатья значения\n",
    "        device (str): Устройство, на котором выполняются вычисления (\"cpu\" или \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Кортеж, содержащий тензоры среднего значения и стандартного отклонения.\n",
    "    \"\"\"\n",
    "    # 1 Создаем список из всех тензоров\n",
    "    inputs_list = []\n",
    "    for inputs, _ in loader:\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "    # 2 Соединяем все тензоры в один большой тензор\n",
    "    all_inputs = torch.cat(inputs_list, dim=0).float().to(device)  # Преобразуем в float и перемещаем на устройство\n",
    "\n",
    "    # 3 Вычисляем mean и std\n",
    "    mean = torch.mean(all_inputs, dim=dim)\n",
    "    std = torch.std(all_inputs, dim=dim)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    A class to evaluate a PyTorch model on given datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: torch.device, task_type: str):\n",
    "        \"\"\"\n",
    "        Initializes the ModelEvaluator.\n",
    "\n",
    "        Args:\n",
    "            model: The PyTorch model to evaluate.\n",
    "            device: The device to perform evaluation on (e.g., 'cuda' or 'cpu').\n",
    "            task_type: The type of problem we will be solving ('CV','NLP')\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        self.model.to(self.device)  # Move model to the specified device\n",
    "\n",
    "        available_task_types = ['CV','NLP']\n",
    "        if self.task_type not in available_task_types:\n",
    "            raise ValueError(f\"task_type can be only: {available_task_types}\")\n",
    "\n",
    "    def evaluate_dataset(self, loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a given dataset and returns a dictionary of metrics.\n",
    "\n",
    "        Args:\n",
    "            loader: The DataLoader for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing evaluation metrics, e.g., {\"accuracy\": 0.95}.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if self.task_type == 'CV':\n",
    "                    inputs, target = batch\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    target = target.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                elif self.task_type == 'NLP':\n",
    "                    inputs = batch['input_ids'].to(self.device)\n",
    "                    target = batch['label'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    outputs = self.model(inputs, attention_mask)  # Pass attention mask\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)  # Get predictions\n",
    "                correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "                total += target.size(0)  # Count total number of samples\n",
    "\n",
    "        accuracy: float = correct / total\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def evaluate(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on train and validation datasets and returns a dictionary of metrics.\n",
    "\n",
    "        Args:\n",
    "            train_loader: The DataLoader for the training dataset.\n",
    "            val_loader: The DataLoader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing evaluation metrics for both training and validation datasets,\n",
    "            e.g., {\"train\": {\"accuracy\": 0.90}, \"val\": {\"accuracy\": 0.95}}.\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on device {self.device}.\")\n",
    "        metrics = {}\n",
    "        metrics[\"train\"] = self.evaluate_dataset(train_loader)\n",
    "        metrics[\"val\"] = self.evaluate_dataset(val_loader)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the training loop for a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        optimizer: The optimizer to use for training.\n",
    "        loss_fn: The loss function to use.\n",
    "        train_loader: The DataLoader for the training data.\n",
    "        device: The device to train on (CPU or GPU).\n",
    "        task_type: The type of problem we will be solving ('CV','NLP')\n",
    "        clip_grad_norm: Optional value to clip gradients to. Defaults to None.\n",
    "        print_interval:  How often to print the training loss. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        loss_fn: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        task_type: str,\n",
    "        clip_grad_norm: Optional[float] = None,\n",
    "        print_interval: int = 10,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_loader = train_loader\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "        self.clip_grad_norm = clip_grad_norm\n",
    "        self.print_interval = print_interval\n",
    "\n",
    "        self.model.to(self.device)  # Move the model to the device in the constructor\n",
    "\n",
    "        available_task_types = ['CV','NLP']\n",
    "        if self.task_type not in available_task_types:\n",
    "            raise ValueError(f\"task_type can be only: {available_task_types}\")\n",
    "\n",
    "    def train_one_batch(self, inputs: torch.Tensor, target: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> float:\n",
    "        \"\"\"Trains the model on a single batch of data.\n",
    "\n",
    "        Args:\n",
    "            inputs: The input tensor.\n",
    "            target: The target tensor.\n",
    "            attention_mask: The attention mask tensor (for NLP)\n",
    "\n",
    "        Returns:\n",
    "            The loss value for the batch.\n",
    "        \"\"\"\n",
    "        self.model.train()  # Ensure the model is in training mode\n",
    "\n",
    "        self.optimizer.zero_grad(set_to_none=True)  # More efficient if possible\n",
    "\n",
    "        if attention_mask is None:\n",
    "            outputs = self.model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "        loss = self.loss_fn(outputs, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)  # Gradient clipping\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train_epoch(self, epoch_num: int = 0) -> float:\n",
    "        \"\"\"Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch_num: The current epoch number (for logging).  Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            The average loss for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()  # Ensure the model is in training mode\n",
    "        loss_train = 0.0\n",
    "        num_batches = len(self.train_loader)\n",
    "        for batch in self.train_loader:\n",
    "            if self.task_type == 'CV':\n",
    "                inputs, target = batch\n",
    "                inputs = inputs.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                loss_train += self.train_one_batch(inputs, target)\n",
    "            elif self.task_type == 'NLP':\n",
    "                inputs = batch['input_ids'].to(self.device)\n",
    "                target = batch['label'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                loss_train += self.train_one_batch(inputs, target, attention_mask)       \n",
    "\n",
    "        return loss_train / num_batches\n",
    "\n",
    "    def training_loop(self, n_epochs: int) -> None:\n",
    "        \"\"\"Executes the main training loop.\n",
    "\n",
    "        Args:\n",
    "            n_epochs: The number of epochs to train for.\n",
    "        \"\"\"\n",
    "        print(f\"Training on device {self.device}.\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            avg_loss = self.train_epoch(epoch)\n",
    "\n",
    "            if epoch == 1 or epoch % self.print_interval == 0:\n",
    "                print(f\"{datetime.datetime.now()} Epoch {epoch}, Training loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1d22b-aadb-4588-861b-ade803ead6cd",
   "metadata": {},
   "source": [
    "# 1. Computer Vision - CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f90f32-029c-4577-85f7-4f508c02a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.2860])\n",
      "Std Dev: tensor([0.3530])\n",
      "Training on device cuda.\n",
      "2025-02-19 18:18:42.583446 Epoch 1, Training loss: 0.5019\n",
      "2025-02-19 18:18:49.462764 Epoch 2, Training loss: 0.2615\n",
      "2025-02-19 18:18:56.760571 Epoch 3, Training loss: 0.2338\n",
      "2025-02-19 18:19:04.282560 Epoch 4, Training loss: 0.2114\n",
      "2025-02-19 18:19:11.639893 Epoch 5, Training loss: 0.1961\n",
      "2025-02-19 18:19:18.987156 Epoch 6, Training loss: 0.1803\n",
      "2025-02-19 18:19:26.302673 Epoch 7, Training loss: 0.1685\n",
      "2025-02-19 18:19:33.582800 Epoch 8, Training loss: 0.1682\n",
      "2025-02-19 18:19:40.917454 Epoch 9, Training loss: 0.1576\n",
      "2025-02-19 18:19:48.209993 Epoch 10, Training loss: 0.1471\n",
      "Evaluating on device cuda.\n",
      "{'train': {'accuracy': 0.9628166666666667}, 'val': {'accuracy': 0.9174}}\n"
     ]
    }
   ],
   "source": [
    "# 0 Создаем словарь с классами\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "class_names_dict = dict(zip(range(len(class_names)), class_names))\n",
    "\n",
    "# 1 Загрузка и подготовка данных (без нормализации), получение Mean и Std\n",
    "trainset= torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]))\n",
    "# 1.1 создаем dataloader\n",
    "dataloader = DataLoader(trainset, batch_size=64, shuffle=False, num_workers=2)\n",
    "# 1.2 считаем mean и std\n",
    "mean, std = compute_mean_and_std(dataloader, [0,2,3])\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std Dev: {std}\")\n",
    "\n",
    "# 2 Загрузка и подготовка данных (с нормализацией)\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])) # Тренировочный набор\n",
    "valset = datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])) # Валидационный набор\n",
    "\n",
    "# 3 Создаем dataloders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# 4 Определяем архитектуру нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,28, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(28,56, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(56*14*14, 112)\n",
    "        self.fc2 = nn.Linear(112, 10)\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(self.dropout1(x),2)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = nn.functional.relu(self.fc1(self.dropout2(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 5 Настройка модели, оптимизатора и функции потерь\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1 Создание и настройка тренера (Trainer)\n",
    "task_type = 'CV'\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    task_type=task_type,\n",
    "    clip_grad_norm=1.0,\n",
    "    print_interval=1,\n",
    ")\n",
    "# 6.2 Запуск процесса обучения\n",
    "trainer.training_loop(n_epochs=10)\n",
    "\n",
    "# 7 Оценка обученной модели\n",
    "evaluator = ModelEvaluator(model, device, task_type)\n",
    "metrics = evaluator.evaluate(train_loader, val_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441ce4f-354c-4bf4-8803-831076f95b12",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da2b5ee-e57c-4596-a4bb-afd2eff3cd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real answer: Coat\n",
      "Model answer: Coat\n",
      "\n",
      "Result:\n",
      "Model response is correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number = 10\n",
    "data, label = valset[number]\n",
    "plt.imshow(data.squeeze(),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# predict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "outputs = model(data.to(device).unsqueeze(0))\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "# validate\n",
    "real_answer = class_names_dict[label]\n",
    "model_answer = class_names_dict[predicted.item()]\n",
    "if real_answer == model_answer:\n",
    "    result = 'correct'\n",
    "else:\n",
    "    result = 'incorrect'\n",
    "    \n",
    "print(f\"\"\"\n",
    "Real answer: {real_answer}\n",
    "Model answer: {model_answer}\n",
    "\n",
    "Result:\n",
    "Model response is {result}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d7f83-78ed-4ce6-a0aa-dc4ff3fef73d",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing (NLP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f36ef7-ea84-405d-8400-b55fba127226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "2025-02-19 18:20:13.334561 Epoch 1, Training loss: 0.6928\n",
      "2025-02-19 18:20:16.489710 Epoch 2, Training loss: 0.6635\n",
      "2025-02-19 18:20:19.793391 Epoch 3, Training loss: 0.5458\n",
      "2025-02-19 18:20:22.976454 Epoch 4, Training loss: 0.4812\n",
      "2025-02-19 18:20:26.145222 Epoch 5, Training loss: 0.4412\n",
      "2025-02-19 18:20:29.294574 Epoch 6, Training loss: 0.4045\n",
      "2025-02-19 18:20:32.487872 Epoch 7, Training loss: 0.3762\n",
      "2025-02-19 18:20:35.668688 Epoch 8, Training loss: 0.3575\n",
      "2025-02-19 18:20:38.857502 Epoch 9, Training loss: 0.3373\n",
      "2025-02-19 18:20:42.052248 Epoch 10, Training loss: 0.3161\n",
      "Evaluating on device cuda.\n",
      "{'train': {'accuracy': 0.88264}, 'val': {'accuracy': 0.83596}}\n"
     ]
    }
   ],
   "source": [
    "# 0 Создаем словарь с классами\n",
    "class_names = ['Negative','Positive']\n",
    "class_names_dict = dict(zip(range(len(class_names)), class_names))\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    # 2. Инициализация токенизатора (использовал BERT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # 4. Функция для токенизации и подготовки данных\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    # 5. Токенизация всего датасета\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    return tokenized_dataset    \n",
    "\n",
    "# 1. Загрузка данных и создание Dataloader\n",
    "batch_size = 64\n",
    "# 1.1 Train\n",
    "trainset = load_dataset(\"imdb\", split=\"train\", cache_dir=\"./data\")\n",
    "train_loader = DataLoader(tokenize_dataset(trainset), batch_size=batch_size, shuffle=True)\n",
    "# 1.2 Test\n",
    "valset = load_dataset(\"imdb\", split=\"test\", cache_dir=\"./data\")\n",
    "val_loader = DataLoader(tokenize_dataset(valset), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. Определение модели (LSTM)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_classes)  # * 2 because bidirectional\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):  # attention_mask is used but not directly in lstm\n",
    "        embedded = self.embedding(input_ids) # [batch_size, seq_len, embedding_dim]\n",
    "        # lstm expects input in the shape (batch_size, seq_len, input_size)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        # take the output of the last time step for each sequence.\n",
    "        # In bidirectional LSTM, the output is a concatenation of the forward and backward hidden states\n",
    "\n",
    "        # Average pooling over the sequence length\n",
    "        pooled_output = torch.mean(lstm_out, dim=1)  # [batch_size, hidden_dim * 2]  (Average over sequence)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 3. Параметры модели\n",
    "vocab_size = AutoTokenizer.from_pretrained('bert-base-uncased').vocab_size  # BERT vocab size (bert-base-uncased)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 64 \n",
    "num_classes = 2\n",
    "lstm_layers = 2 \n",
    "dropout = 0.5\n",
    "\n",
    "# 5 Настройка модели, оптимизатора и функции потерь\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, dropout).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1 Создание и настройка тренера (Trainer)\n",
    "task_type = 'NLP'\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    task_type=task_type,\n",
    "    clip_grad_norm=1.0,\n",
    "    print_interval=1,\n",
    ")\n",
    "# 6.2 Запуск процесса обучения\n",
    "trainer.training_loop(n_epochs=10)\n",
    "\n",
    "# 7 Оценка обученной модели\n",
    "evaluator = ModelEvaluator(model, device, task_type)\n",
    "metrics = evaluator.evaluate(train_loader, val_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373ac78-2c8f-4643-ac34-75ca2c25422c",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7274dbdf-2398-4d91-bc2c-4785396e662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This flick is a waste of time.I expect from an action movie to have more than 2 explosions and some shooting.Van Damme's acting is awful. He never was much of an actor, but here it is worse.He was definitely better in his earlier movies. His screenplay part for the whole movie was probably not more than one page of stupid nonsense one liners.The whole dialog in the film is a disaster, same as the plot.The title \"The Shepherd\" makes no sense. Why didn't they just call it \"Border patrol\"? The fighting scenes could have been better, but either they weren't able to afford it, or the fighting choreographer was suffering from lack of ideas.This is a cheap low type of action cinema.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aff4d903d8d4e7ea8aef04930c60771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real answer: Negative\n",
      "Model answer: Negative\n",
      "\n",
      "Result:\n",
      "Model response is correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number = 10\n",
    "data = valset[number]\n",
    "print(data['text'])\n",
    "dataset_dict = {\n",
    "    \"text\": [data[\"text\"]], \n",
    "    \"label\": [data[\"label\"]]}\n",
    "single_dataset = Dataset.from_dict(dataset_dict)\n",
    "single_dataset = tokenize_dataset(single_dataset)\n",
    "\n",
    "inputs = single_dataset['input_ids'].to(device)\n",
    "target = single_dataset['label'].to(device)\n",
    "attention_mask = single_dataset['attention_mask'].to(device)\n",
    "\n",
    "# predict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "outputs = model(inputs, attention_mask)\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "# validate\n",
    "real_answer = class_names_dict[int(target[0])]\n",
    "model_answer = class_names_dict[int(predicted[0])]\n",
    "if real_answer == model_answer:\n",
    "    result = 'correct'\n",
    "else:\n",
    "    result = 'incorrect'\n",
    "    \n",
    "print(f\"\"\"\n",
    "Real answer: {real_answer}\n",
    "Model answer: {model_answer}\n",
    "\n",
    "Result:\n",
    "Model response is {result}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38961637-b381-4214-9404-ac8c8bd2b2b2",
   "metadata": {},
   "source": [
    "# Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaa83cd-7279-4444-bb9d-a8ef36d3d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "2025-02-19 18:20:44.809687 Epoch 1, Training loss: 1.0292\n",
      "2025-02-19 18:20:44.818194 Epoch 2, Training loss: 0.9183\n",
      "2025-02-19 18:20:44.825193 Epoch 3, Training loss: 0.7900\n",
      "2025-02-19 18:20:44.833698 Epoch 4, Training loss: 0.6493\n",
      "2025-02-19 18:20:44.841709 Epoch 5, Training loss: 0.5306\n",
      "2025-02-19 18:20:44.850213 Epoch 6, Training loss: 0.4577\n",
      "2025-02-19 18:20:44.857721 Epoch 7, Training loss: 0.3950\n",
      "2025-02-19 18:20:44.863721 Epoch 8, Training loss: 0.3419\n",
      "2025-02-19 18:20:44.870231 Epoch 9, Training loss: 0.2898\n",
      "2025-02-19 18:20:44.876793 Epoch 10, Training loss: 0.2436\n",
      "Evaluating on device cuda.\n",
      "{'train': {'accuracy': 0.9333333333333333}, 'val': {'accuracy': 0.9333333333333333}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "# 0 Создаем словарь с классами\n",
    "class_names = ['setosa', 'versicolor', 'virginica']\n",
    "class_names_dict = dict(zip(range(len(class_names)), class_names))\n",
    "\n",
    "# 1. Загрузка и подготовка данных\n",
    "iris = load_iris()\n",
    "iris_data = iris['data']\n",
    "iris_target = iris['target']\n",
    "\n",
    "# 2. Разделение на train и val\n",
    "train_data, val_data, train_target, val_target = train_test_split(\n",
    "    iris_data, iris_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Нормализация данных (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data)\n",
    "val_data_normalized = scaler.transform(val_data)\n",
    "\n",
    "# 4. Создание кастомного Dataset\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.targets = targets.astype(np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "# 5. Создание Dataset\n",
    "trainset = IrisDataset(train_data_normalized, train_target)\n",
    "valset = IrisDataset(val_data_normalized, val_target)\n",
    "\n",
    "# 5. Создание DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4 Определяем архитектуру нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8) # 4 входных признака, 10 нейронов в первом слое\n",
    "        self.fc2 = nn.Linear(8, 16) # 4 входных признака, 10 нейронов в первом слое\n",
    "        self.fc3 = nn.Linear(16, 3) # 10 нейронов, 3 выходных класса\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 5 Настройка модели, оптимизатора и функции потерь\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1 Создание и настройка тренера (Trainer)\n",
    "task_type = 'CV'\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    task_type=task_type,\n",
    "    clip_grad_norm=1.0,\n",
    "    print_interval=1,\n",
    ")\n",
    "# 6.2 Запуск процесса обучения\n",
    "trainer.training_loop(n_epochs=10)\n",
    "\n",
    "# 7 Оценка обученной модели\n",
    "evaluator = ModelEvaluator(model, device, task_type)\n",
    "metrics = evaluator.evaluate(train_loader, val_loader)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c4fe8-6628-48d3-a6c6-dcad9aea96db",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688fef61-f6c0-4699-bfd9-dd6238d30635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8421045 0.3092991 0.7869979 1.0901277]\n",
      "\n",
      "Real answer: virginica\n",
      "Model answer: virginica\n",
      "\n",
      "Result:\n",
      "Model response is correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number = 10\n",
    "data, label = valset[number]\n",
    "print(data)\n",
    "\n",
    "# predict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "data = torch.from_numpy(data).float()\n",
    "outputs = model(data.to(device).unsqueeze(0))\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "# validate\n",
    "real_answer = class_names_dict[label]\n",
    "model_answer = class_names_dict[predicted.item()]\n",
    "if real_answer == model_answer:\n",
    "    result = 'correct'\n",
    "else:\n",
    "    result = 'incorrect'\n",
    "    \n",
    "print(f\"\"\"\n",
    "Real answer: {real_answer}\n",
    "Model answer: {model_answer}\n",
    "\n",
    "Result:\n",
    "Model response is {result}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b745e2-b2ce-4c6c-956d-96dff0cbd571",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8d485-2fd5-4856-b2c6-724c4bf985ff",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7722f-bcf1-4d83-8165-15f24444d9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50b1b6c-ace4-4f82-a6d7-335f24451851",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08efd0d-c21c-4df8-83ac-7a60d9708435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18bcc64f-1239-453a-8832-08f035b8eba6",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b70c6-1781-4f1f-b391-3d54d14e4186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
