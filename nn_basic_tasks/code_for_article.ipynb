{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136c0b0-83ce-4233-b64b-f6642de8f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=q_HS4s1L8UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626dc3d4-cc86-42a6-afcd-f8f055ebbd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on device cuda.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import datetime\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split  # Импортируем train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Выбираем устройство для обучения (GPU, если доступен, иначе CPU)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training and evaluating on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f82dee66-bcf9-4dd0-8d0c-fbbbfc5530dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_and_std(loader: DataLoader, dim: list, device=\"cpu\") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Вычисление среднего значения и стандартного отклонения в наборе данных.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader для итерации по набору данных\n",
    "        dim (list): Размерности, по которым будут считатья значения\n",
    "        device (str): Устройство, на котором выполняются вычисления (\"cpu\" или \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Кортеж, содержащий тензоры среднего значения и стандартного отклонения.\n",
    "    \"\"\"\n",
    "    # 1 Создаем список из всех тензоров\n",
    "    inputs_list = []\n",
    "    for inputs, _ in loader:\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "    # 2 Соединяем все тензоры в один большой тензор\n",
    "    all_inputs = torch.cat(inputs_list, dim=0).float().to(device)  # Преобразуем в float и перемещаем на устройство\n",
    "\n",
    "    # 3 Вычисляем mean и std\n",
    "    mean = torch.mean(all_inputs, dim=dim)\n",
    "    std = torch.std(all_inputs, dim=dim)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    A class to evaluate a PyTorch model on given datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: torch.device, task_type: str):\n",
    "        \"\"\"\n",
    "        Initializes the ModelEvaluator.\n",
    "\n",
    "        Args:\n",
    "            model: The PyTorch model to evaluate.\n",
    "            device: The device to perform evaluation on (e.g., 'cuda' or 'cpu').\n",
    "            task_type: The type of problem we will be solving ('CV','NLP')\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        self.model.to(self.device)  # Move model to the specified device\n",
    "\n",
    "        available_task_types = ['CV','NLP']\n",
    "        if self.task_type not in available_task_types:\n",
    "            raise ValueError(f\"task_type can be only: {available_task_types}\")\n",
    "\n",
    "    def evaluate_dataset(self, loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a given dataset and returns a dictionary of metrics.\n",
    "\n",
    "        Args:\n",
    "            loader: The DataLoader for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing evaluation metrics, e.g., {\"accuracy\": 0.95}.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if self.task_type == 'CV':\n",
    "                    inputs, target = batch\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    target = target.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                elif self.task_type == 'NLP':\n",
    "                    inputs = batch['input_ids'].to(self.device)\n",
    "                    target = batch['label'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    outputs = self.model(inputs, attention_mask)  # Pass attention mask\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)  # Get predictions\n",
    "                correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "                total += target.size(0)  # Count total number of samples\n",
    "\n",
    "        accuracy: float = correct / total\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def evaluate(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on train and validation datasets and returns a dictionary of metrics.\n",
    "\n",
    "        Args:\n",
    "            train_loader: The DataLoader for the training dataset.\n",
    "            val_loader: The DataLoader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing evaluation metrics for both training and validation datasets,\n",
    "            e.g., {\"train\": {\"accuracy\": 0.90}, \"val\": {\"accuracy\": 0.95}}.\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on device {self.device}.\")\n",
    "        metrics = {}\n",
    "        metrics[\"train\"] = self.evaluate_dataset(train_loader)\n",
    "        metrics[\"val\"] = self.evaluate_dataset(val_loader)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    A class to encapsulate the training loop for a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        optimizer: The optimizer to use for training.\n",
    "        loss_fn: The loss function to use.\n",
    "        train_loader: The DataLoader for the training data.\n",
    "        device: The device to train on (CPU or GPU).\n",
    "        task_type: The type of problem we will be solving ('CV','NLP')\n",
    "        clip_grad_norm: Optional value to clip gradients to. Defaults to None.\n",
    "        print_interval:  How often to print the training loss. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        loss_fn: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        task_type: str,\n",
    "        clip_grad_norm: Optional[float] = None,\n",
    "        print_interval: int = 10,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_loader = train_loader\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "        self.clip_grad_norm = clip_grad_norm\n",
    "        self.print_interval = print_interval\n",
    "\n",
    "        self.model.to(self.device)  # Move the model to the device in the constructor\n",
    "\n",
    "        available_task_types = ['CV','NLP']\n",
    "        if self.task_type not in available_task_types:\n",
    "            raise ValueError(f\"task_type can be only: {available_task_types}\")\n",
    "\n",
    "    def train_one_batch(self, inputs: torch.Tensor, target: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> float:\n",
    "        \"\"\"Trains the model on a single batch of data.\n",
    "\n",
    "        Args:\n",
    "            inputs: The input tensor.\n",
    "            target: The target tensor.\n",
    "            attention_mask: The attention mask tensor (for NLP)\n",
    "\n",
    "        Returns:\n",
    "            The loss value for the batch.\n",
    "        \"\"\"\n",
    "        self.model.train()  # Ensure the model is in training mode\n",
    "\n",
    "        self.optimizer.zero_grad(set_to_none=True)  # More efficient if possible\n",
    "\n",
    "        if attention_mask is None:\n",
    "            outputs = self.model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "        loss = self.loss_fn(outputs, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)  # Gradient clipping\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train_epoch(self, epoch_num: int = 0) -> float:\n",
    "        \"\"\"Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch_num: The current epoch number (for logging).  Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            The average loss for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()  # Ensure the model is in training mode\n",
    "        loss_train = 0.0\n",
    "        num_batches = len(self.train_loader)\n",
    "        for batch in self.train_loader:\n",
    "            if self.task_type == 'CV':\n",
    "                inputs, target = batch\n",
    "                inputs = inputs.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                loss_train += self.train_one_batch(inputs, target)\n",
    "            elif self.task_type == 'NLP':\n",
    "                inputs = batch['input_ids'].to(self.device)\n",
    "                target = batch['label'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                loss_train += self.train_one_batch(inputs, target, attention_mask)       \n",
    "\n",
    "        return loss_train / num_batches\n",
    "\n",
    "    def training_loop(self, n_epochs: int) -> None:\n",
    "        \"\"\"Executes the main training loop.\n",
    "\n",
    "        Args:\n",
    "            n_epochs: The number of epochs to train for.\n",
    "        \"\"\"\n",
    "        print(f\"Training on device {self.device}.\")\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            avg_loss = self.train_epoch(epoch)\n",
    "\n",
    "            if epoch == 1 or epoch % self.print_interval == 0:\n",
    "                print(f\"{datetime.datetime.now()} Epoch {epoch}, Training loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1d22b-aadb-4588-861b-ade803ead6cd",
   "metadata": {},
   "source": [
    "# 1. Computer Vision - CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f90f32-029c-4577-85f7-4f508c02a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.2860])\n",
      "Std Dev: tensor([0.3530])\n",
      "Training on device cuda.\n",
      "2025-02-19 18:18:42.583446 Epoch 1, Training loss: 0.5019\n",
      "2025-02-19 18:18:49.462764 Epoch 2, Training loss: 0.2615\n",
      "2025-02-19 18:18:56.760571 Epoch 3, Training loss: 0.2338\n",
      "2025-02-19 18:19:04.282560 Epoch 4, Training loss: 0.2114\n",
      "2025-02-19 18:19:11.639893 Epoch 5, Training loss: 0.1961\n",
      "2025-02-19 18:19:18.987156 Epoch 6, Training loss: 0.1803\n",
      "2025-02-19 18:19:26.302673 Epoch 7, Training loss: 0.1685\n",
      "2025-02-19 18:19:33.582800 Epoch 8, Training loss: 0.1682\n",
      "2025-02-19 18:19:40.917454 Epoch 9, Training loss: 0.1576\n",
      "2025-02-19 18:19:48.209993 Epoch 10, Training loss: 0.1471\n",
      "Evaluating on device cuda.\n",
      "{'train': {'accuracy': 0.9628166666666667}, 'val': {'accuracy': 0.9174}}\n"
     ]
    }
   ],
   "source": [
    "# 0 Создаем словарь с классами\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "class_names_dict = dict(zip(range(len(class_names)), class_names))\n",
    "\n",
    "# 1 Загрузка и подготовка данных (без нормализации), получение Mean и Std\n",
    "trainset= torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]))\n",
    "# 1.1 создаем dataloader\n",
    "dataloader = DataLoader(trainset, batch_size=64, shuffle=False, num_workers=2)\n",
    "# 1.2 считаем mean и std\n",
    "mean, std = compute_mean_and_std(dataloader, [0,2,3])\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std Dev: {std}\")\n",
    "\n",
    "# 2 Загрузка и подготовка данных (с нормализацией)\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])) # Тренировочный набор\n",
    "valset = datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])) # Валидационный набор\n",
    "\n",
    "# 3 Создаем dataloders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# 4 Определяем архитектуру нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,28, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(28,56, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(56*14*14, 112)\n",
    "        self.fc2 = nn.Linear(112, 10)\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(self.dropout1(x),2)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = nn.functional.relu(self.fc1(self.dropout2(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 5 Настройка модели, оптимизатора и функции потерь\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1 Создание и настройка тренера (Trainer)\n",
    "task_type = 'CV'\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    task_type=task_type,\n",
    "    clip_grad_norm=1.0,\n",
    "    print_interval=1,\n",
    ")\n",
    "# 6.2 Запуск процесса обучения\n",
    "trainer.training_loop(n_epochs=10)\n",
    "\n",
    "# 7 Оценка обученной модели\n",
    "evaluator = ModelEvaluator(model, device, task_type)\n",
    "metrics = evaluator.evaluate(train_loader, val_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441ce4f-354c-4bf4-8803-831076f95b12",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da2b5ee-e57c-4596-a4bb-afd2eff3cd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real answer: Coat\n",
      "Model answer: Coat\n",
      "\n",
      "Result:\n",
      "Model response is correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number = 10\n",
    "data, label = valset[number]\n",
    "plt.imshow(data.squeeze(),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# predict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "outputs = model(data.to(device).unsqueeze(0))\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "# validate\n",
    "real_answer = class_names_dict[label]\n",
    "model_answer = class_names_dict[predicted.item()]\n",
    "if real_answer == model_answer:\n",
    "    result = 'correct'\n",
    "else:\n",
    "    result = 'incorrect'\n",
    "    \n",
    "print(f\"\"\"\n",
    "Real answer: {real_answer}\n",
    "Model answer: {model_answer}\n",
    "\n",
    "Result:\n",
    "Model response is {result}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d7f83-78ed-4ce6-a0aa-dc4ff3fef73d",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing (NLP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f36ef7-ea84-405d-8400-b55fba127226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "2025-02-19 18:20:13.334561 Epoch 1, Training loss: 0.6928\n",
      "2025-02-19 18:20:16.489710 Epoch 2, Training loss: 0.6635\n",
      "2025-02-19 18:20:19.793391 Epoch 3, Training loss: 0.5458\n",
      "2025-02-19 18:20:22.976454 Epoch 4, Training loss: 0.4812\n",
      "2025-02-19 18:20:26.145222 Epoch 5, Training loss: 0.4412\n",
      "2025-02-19 18:20:29.294574 Epoch 6, Training loss: 0.4045\n",
      "2025-02-19 18:20:32.487872 Epoch 7, Training loss: 0.3762\n",
      "2025-02-19 18:20:35.668688 Epoch 8, Training loss: 0.3575\n",
      "2025-02-19 18:20:38.857502 Epoch 9, Training loss: 0.3373\n",
      "2025-02-19 18:20:42.052248 Epoch 10, Training loss: 0.3161\n",
      "Evaluating on device cuda.\n",
      "{'train': {'accuracy': 0.88264}, 'val': {'accuracy': 0.83596}}\n"
     ]
    }
   ],
   "source": [
    "# 0 Создаем словарь с классами\n",
    "class_names = ['Negative','Positive']\n",
    "class_names_dict = dict(zip(range(len(class_names)), class_names))\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    # 2. Инициализация токенизатора (использовал BERT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # 4. Функция для токенизации и подготовки данных\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    # 5. Токенизация всего датасета\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    return tokenized_dataset    \n",
    "\n",
    "# 1. Загрузка данных и создание Dataloader\n",
    "batch_size = 64\n",
    "# 1.1 Train\n",
    "trainset = load_dataset(\"imdb\", split=\"train\", cache_dir=\"./data\")\n",
    "train_loader = DataLoader(tokenize_dataset(trainset), batch_size=batch_size, shuffle=True)\n",
    "# 1.2 Test\n",
    "valset = load_dataset(\"imdb\", split=\"test\", cache_dir=\"./data\")\n",
    "val_loader = DataLoader(tokenize_dataset(valset), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. Определение модели (LSTM)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_classes)  # * 2 because bidirectional\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):  # attention_mask is used but not directly in lstm\n",
    "        embedded = self.embedding(input_ids) # [batch_size, seq_len, embedding_dim]\n",
    "        # lstm expects input in the shape (batch_size, seq_len, input_size)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        # take the output of the last time step for each sequence.\n",
    "        # In bidirectional LSTM, the output is a concatenation of the forward and backward hidden states\n",
    "\n",
    "        # Average pooling over the sequence length\n",
    "        pooled_output = torch.mean(lstm_out, dim=1)  # [batch_size, hidden_dim * 2]  (Average over sequence)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 3. Параметры модели\n",
    "vocab_size = AutoTokenizer.from_pretrained('bert-base-uncased').vocab_size  # BERT vocab size (bert-base-uncased)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 64 \n",
    "num_classes = 2\n",
    "lstm_layers = 2 \n",
    "dropout = 0.5\n",
    "\n",
    "# 5 Настройка модели, оптимизатора и функции потерь\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, dropout).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1 Создание и настройка тренера (Trainer)\n",
    "task_type = 'NLP'\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    task_type=task_type,\n",
    "    clip_grad_norm=1.0,\n",
    "    print_interval=1,\n",
    ")\n",
    "# 6.2 Запуск процесса обучения\n",
    "trainer.training_loop(n_epochs=10)\n",
    "\n",
    "# 7 Оценка обученной модели\n",
    "evaluator = ModelEvaluator(model, device, task_type)\n",
    "metrics = evaluator.evaluate(train_loader, val_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373ac78-2c8f-4643-ac34-75ca2c25422c",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7274dbdf-2398-4d91-bc2c-4785396e662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This flick is a waste of time.I expect from an action movie to have more than 2 explosions and some shooting.Van Damme's acting is awful. He never was much of an actor, but here it is worse.He was definitely better in his earlier movies. His screenplay part for the whole movie was probably not more than one page of stupid nonsense one liners.The whole dialog in the film is a disaster, same as the plot.The title \"The Shepherd\" makes no sense. Why didn't they just call it \"Border patrol\"? The fighting scenes could have been better, but either they weren't able to afford it, or the fighting choreographer was suffering from lack of ideas.This is a cheap low type of action cinema.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aff4d903d8d4e7ea8aef04930c60771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real answer: Negative\n",
      "Model answer: Negative\n",
      "\n",
      "Result:\n",
      "Model response is correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number = 10\n",
    "data = valset[number]\n",
    "print(data['text'])\n",
    "dataset_dict = {\n",
    "    \"text\": [data[\"text\"]], \n",
    "    \"label\": [data[\"label\"]]}\n",
    "single_dataset = Dataset.from_dict(dataset_dict)\n",
    "single_dataset = tokenize_dataset(single_dataset)\n",
    "\n",
    "inputs = single_dataset['input_ids'].to(device)\n",
    "target = single_dataset['label'].to(device)\n",
    "attention_mask = single_dataset['attention_mask'].to(device)\n",
    "\n",
    "# predict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "outputs = model(inputs, attention_mask)\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "# validate\n",
    "real_answer = class_names_dict[int(target[0])]\n",
    "model_answer = class_names_dict[int(predicted[0])]\n",
    "if real_answer == model_answer:\n",
    "    result = 'correct'\n",
    "else:\n",
    "    result = 'incorrect'\n",
    "    \n",
    "print(f\"\"\"\n",
    "Real answer: {real_answer}\n",
    "Model answer: {model_answer}\n",
    "\n",
    "Result:\n",
    "Model response is {result}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38961637-b381-4214-9404-ac8c8bd2b2b2",
   "metadata": {},
   "source": [
    "# Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6aaa83cd-7279-4444-bb9d-a8ef36d3d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "2025-02-20 14:06:01.327994 Epoch 1, Training loss: 1.0783\n",
      "2025-02-20 14:06:01.354296 Epoch 2, Training loss: 1.0171\n",
      "2025-02-20 14:06:01.361299 Epoch 3, Training loss: 0.9513\n",
      "2025-02-20 14:06:01.368297 Epoch 4, Training loss: 0.8550\n",
      "2025-02-20 14:06:01.372256 Epoch 5, Training loss: 0.7687\n",
      "2025-02-20 14:06:01.396905 Epoch 6, Training loss: 0.6757\n",
      "2025-02-20 14:06:01.403271 Epoch 7, Training loss: 0.5332\n",
      "2025-02-20 14:06:01.410270 Epoch 8, Training loss: 0.5026\n",
      "2025-02-20 14:06:01.416272 Epoch 9, Training loss: 0.3796\n",
      "2025-02-20 14:06:01.423271 Epoch 10, Training loss: 0.3176\n",
      "Evaluating on device cuda.\n",
      "{'train': {'accuracy': 0.925}, 'val': {'accuracy': 0.9666666666666667}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "# 0 Создаем словарь с классами\n",
    "class_names = ['setosa', 'versicolor', 'virginica']\n",
    "class_names_dict = dict(zip(range(len(class_names)), class_names))\n",
    "\n",
    "# 1. Загрузка и подготовка данных\n",
    "iris = load_iris()\n",
    "iris_data = iris['data']\n",
    "iris_target = iris['target']\n",
    "\n",
    "# 2. Разделение на train и val\n",
    "train_data, val_data, train_target, val_target = train_test_split(\n",
    "    iris_data, iris_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Нормализация данных (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data)\n",
    "val_data_normalized = scaler.transform(val_data)\n",
    "\n",
    "# 4. Создание кастомного Dataset\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.targets = targets.astype(np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "# 5. Создание Dataset\n",
    "trainset = IrisDataset(train_data_normalized, train_target)\n",
    "valset = IrisDataset(val_data_normalized, val_target)\n",
    "\n",
    "# 5. Создание DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4 Определяем архитектуру нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8) \n",
    "        self.fc2 = nn.Linear(8, 16)\n",
    "        self.fc3 = nn.Linear(16, 3)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 5 Настройка модели, оптимизатора и функции потерь\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6.1 Создание и настройка тренера (Trainer)\n",
    "task_type = 'CV'\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    device=device,\n",
    "    task_type=task_type,\n",
    "    clip_grad_norm=1.0,\n",
    "    print_interval=1,\n",
    ")\n",
    "# 6.2 Запуск процесса обучения\n",
    "trainer.training_loop(n_epochs=10)\n",
    "\n",
    "# 7 Оценка обученной модели\n",
    "evaluator = ModelEvaluator(model, device, task_type)\n",
    "metrics = evaluator.evaluate(train_loader, val_loader)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c4fe8-6628-48d3-a6c6-dcad9aea96db",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "688fef61-f6c0-4699-bfd9-dd6238d30635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8421045 0.3092991 0.7869979 1.0901277]\n",
      "\n",
      "Real answer: virginica\n",
      "Model answer: virginica\n",
      "\n",
      "Result:\n",
      "Model response is correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number = 10\n",
    "data, label = valset[number]\n",
    "print(data)\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    data = torch.from_numpy(data).float()\n",
    "    outputs = model(data.to(device).unsqueeze(0))\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "# validate\n",
    "real_answer = class_names_dict[label]\n",
    "model_answer = class_names_dict[predicted.item()]\n",
    "if real_answer == model_answer:\n",
    "    result = 'correct'\n",
    "else:\n",
    "    result = 'incorrect'\n",
    "    \n",
    "print(f\"\"\"\n",
    "Real answer: {real_answer}\n",
    "Model answer: {model_answer}\n",
    "\n",
    "Result:\n",
    "Model response is {result}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b745e2-b2ce-4c6c-956d-96dff0cbd571",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9f1831bd-1c80-466b-92fb-3503ebbdb7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Скачивание данных с yfinance\n",
    "ticker = \"GOOG\"\n",
    "start_date = \"2023-12-10\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "try:\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при скачивании данных: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Нормализация данных (масштабирование к диапазону [0, 1])\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 3. Создание последовательностей\n",
    "def create_sequences(data, seq_length, target_index):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length, target_index]  # Предсказываем только Close\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 2\n",
    "\n",
    "\n",
    "X, y = create_sequences(data.to_numpy(), seq_length, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0ef91e1b-b611-43e4-a811-0b1e812ed121",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[238], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Нормализация данных (масштабирование к диапазону [0, 1])\u001b[39;00m\n\u001b[0;32m     11\u001b[0m scaler_x \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 12\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler_x\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m     13\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler_x\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     15\u001b[0m scaler_y \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    489\u001b[0m     X,\n\u001b[0;32m    490\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[0;32m    491\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    492\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )\n\u001b[0;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml-models\\Lib\\site-packages\\sklearn\\utils\\validation.py:1101\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m     )\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m   1107\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1108\u001b[0m         array,\n\u001b[0;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1112\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "split_index = int(len(X) * (1 - test_size))\n",
    "\n",
    "X_train = X[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Нормализация данных (масштабирование к диапазону [0, 1])\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(X_train)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train = scaler_x.fit_transform(y_train)\n",
    "y_test = scaler_x.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bba1bf16-8de4-42a0-a1ad-0818d8bb6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.34217743e+02, 1.34307417e+02, 1.32414226e+02, 1.33340904e+02,\n",
       "         2.45029000e+07],\n",
       "        [1.33161530e+02, 1.34058301e+02, 1.32354432e+02, 1.32792859e+02,\n",
       "         2.65840000e+07]],\n",
       "\n",
       "       [[1.33161530e+02, 1.34058301e+02, 1.32354432e+02, 1.32792859e+02,\n",
       "         2.65840000e+07],\n",
       "        [1.33490372e+02, 1.34297469e+02, 1.32474019e+02, 1.34063310e+02,\n",
       "         2.54145000e+07]],\n",
       "\n",
       "       [[1.33490372e+02, 1.34297469e+02, 1.32474019e+02, 1.34063310e+02,\n",
       "         2.54145000e+07],\n",
       "        [1.32723114e+02, 1.34551551e+02, 1.30590776e+02, 1.34287500e+02,\n",
       "         2.96191000e+07]]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d1d33eee-f99a-4ed0-993b-bfc8f925d8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b302fab3-dd43-4868-961d-73b7145b2efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-11</th>\n",
       "      <td>134.217743</td>\n",
       "      <td>134.307417</td>\n",
       "      <td>132.414226</td>\n",
       "      <td>133.340904</td>\n",
       "      <td>24502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-12</th>\n",
       "      <td>133.161530</td>\n",
       "      <td>134.058301</td>\n",
       "      <td>132.354432</td>\n",
       "      <td>132.792859</td>\n",
       "      <td>26584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-13</th>\n",
       "      <td>133.490372</td>\n",
       "      <td>134.297469</td>\n",
       "      <td>132.474019</td>\n",
       "      <td>134.063310</td>\n",
       "      <td>25414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-14</th>\n",
       "      <td>132.723114</td>\n",
       "      <td>134.551551</td>\n",
       "      <td>130.590776</td>\n",
       "      <td>134.287500</td>\n",
       "      <td>29619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15</th>\n",
       "      <td>133.360809</td>\n",
       "      <td>134.347270</td>\n",
       "      <td>132.155150</td>\n",
       "      <td>132.444105</td>\n",
       "      <td>58569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-18</th>\n",
       "      <td>136.698822</td>\n",
       "      <td>137.884564</td>\n",
       "      <td>133.291068</td>\n",
       "      <td>133.380743</td>\n",
       "      <td>25699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-19</th>\n",
       "      <td>137.605576</td>\n",
       "      <td>138.273175</td>\n",
       "      <td>136.957894</td>\n",
       "      <td>137.505928</td>\n",
       "      <td>20661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-20</th>\n",
       "      <td>139.159988</td>\n",
       "      <td>142.565750</td>\n",
       "      <td>138.910883</td>\n",
       "      <td>139.827588</td>\n",
       "      <td>33507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-21</th>\n",
       "      <td>141.292328</td>\n",
       "      <td>141.521500</td>\n",
       "      <td>139.970083</td>\n",
       "      <td>140.266017</td>\n",
       "      <td>18101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>142.209030</td>\n",
       "      <td>142.737131</td>\n",
       "      <td>141.546402</td>\n",
       "      <td>141.621146</td>\n",
       "      <td>18494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>142.308670</td>\n",
       "      <td>143.429642</td>\n",
       "      <td>141.989808</td>\n",
       "      <td>142.468086</td>\n",
       "      <td>11170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>140.933624</td>\n",
       "      <td>142.806898</td>\n",
       "      <td>140.546009</td>\n",
       "      <td>142.318647</td>\n",
       "      <td>17288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>140.774185</td>\n",
       "      <td>141.760646</td>\n",
       "      <td>140.323808</td>\n",
       "      <td>141.342152</td>\n",
       "      <td>12192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>140.425430</td>\n",
       "      <td>140.928627</td>\n",
       "      <td>139.399119</td>\n",
       "      <td>140.176325</td>\n",
       "      <td>14872700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            Close        High         Low        Open    Volume\n",
       "Ticker            GOOG        GOOG        GOOG        GOOG      GOOG\n",
       "Date                                                                \n",
       "2023-12-11  134.217743  134.307417  132.414226  133.340904  24502900\n",
       "2023-12-12  133.161530  134.058301  132.354432  132.792859  26584000\n",
       "2023-12-13  133.490372  134.297469  132.474019  134.063310  25414500\n",
       "2023-12-14  132.723114  134.551551  130.590776  134.287500  29619100\n",
       "2023-12-15  133.360809  134.347270  132.155150  132.444105  58569400\n",
       "2023-12-18  136.698822  137.884564  133.291068  133.380743  25699800\n",
       "2023-12-19  137.605576  138.273175  136.957894  137.505928  20661000\n",
       "2023-12-20  139.159988  142.565750  138.910883  139.827588  33507300\n",
       "2023-12-21  141.292328  141.521500  139.970083  140.266017  18101500\n",
       "2023-12-22  142.209030  142.737131  141.546402  141.621146  18494700\n",
       "2023-12-26  142.308670  143.429642  141.989808  142.468086  11170100\n",
       "2023-12-27  140.933624  142.806898  140.546009  142.318647  17288400\n",
       "2023-12-28  140.774185  141.760646  140.323808  141.342152  12192500\n",
       "2023-12-29  140.425430  140.928627  139.399119  140.176325  14872700"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "19b8c26c-2dbc-44c9-8a80-feacce15d490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([133.4903717 , 132.72311401, 133.36080933, 136.69882202,\n",
       "       137.60557556, 139.1599884 , 141.29232788, 142.20903015])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e33966eb-4e97-4c57-ac9e-d0efe4f6ca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([142.30867004, 140.93362427, 140.77418518])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0fd6ddc1-3e8d-4b1e-9a68-4f98ee155383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.39159988e+02, 1.42565750e+02, 1.38910883e+02, 1.39827588e+02,\n",
       "        3.35073000e+07],\n",
       "       [1.41292328e+02, 1.41521500e+02, 1.39970083e+02, 1.40266017e+02,\n",
       "        1.81015000e+07]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f32b99c1-8ad5-470a-9b94-45c5d7b7673e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.34217743e+02, 1.34307417e+02, 1.32414226e+02, 1.33340904e+02,\n",
       "        2.45029000e+07],\n",
       "       [1.33161530e+02, 1.34058301e+02, 1.32354432e+02, 1.32792859e+02,\n",
       "        2.65840000e+07],\n",
       "       [1.33490372e+02, 1.34297469e+02, 1.32474019e+02, 1.34063310e+02,\n",
       "        2.54145000e+07],\n",
       "       [1.32723114e+02, 1.34551551e+02, 1.30590776e+02, 1.34287500e+02,\n",
       "        2.96191000e+07],\n",
       "       [1.33360809e+02, 1.34347270e+02, 1.32155150e+02, 1.32444105e+02,\n",
       "        5.85694000e+07]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "86c48861-dd52-4316-8b2a-28ea4622a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6661e79f-586c-4c35-bc0c-931950efa171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15592511, 0.02658273, 0.15996527, 0.08946534, 0.28128685],\n",
       "       [0.0457371 , 0.        , 0.15471978, 0.03479198, 0.32519257],\n",
       "       [0.08004311, 0.02552123, 0.16521077, 0.16153314, 0.30051921],\n",
       "       [0.        , 0.05263386, 0.        , 0.18389854, 0.38922516],\n",
       "       [0.06652669, 0.03083541, 0.13723742, 0.        , 1.        ],\n",
       "       [0.41476029, 0.40829404, 0.23688785, 0.09343968, 0.30653828],\n",
       "       [0.50935611, 0.44976207, 0.55856648, 0.5049713 , 0.20023291],\n",
       "       [0.6715181 , 0.90781557, 0.72989593, 0.73658191, 0.4712559 ],\n",
       "       [0.8939715 , 0.79638538, 0.82281606, 0.78031991, 0.14623423],\n",
       "       [0.9896052 , 0.92610334, 0.96110144, 0.91550867, 0.15452971],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.        ],\n",
       "       [0.85655023, 0.93354806, 0.87334021, 0.98509191, 0.12907997],\n",
       "       [0.83991697, 0.82190425, 0.85384717, 0.88767597, 0.02156994],\n",
       "       [0.80353359, 0.7331209 , 0.77272726, 0.77137223, 0.07811508]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dbdf15a7-dbd4-422b-8f25-ae88978226d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-11</th>\n",
       "      <td>134.217743</td>\n",
       "      <td>134.307417</td>\n",
       "      <td>132.414226</td>\n",
       "      <td>133.340904</td>\n",
       "      <td>24502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-12</th>\n",
       "      <td>133.161530</td>\n",
       "      <td>134.058301</td>\n",
       "      <td>132.354432</td>\n",
       "      <td>132.792859</td>\n",
       "      <td>26584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-13</th>\n",
       "      <td>133.490372</td>\n",
       "      <td>134.297469</td>\n",
       "      <td>132.474019</td>\n",
       "      <td>134.063310</td>\n",
       "      <td>25414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-14</th>\n",
       "      <td>132.723114</td>\n",
       "      <td>134.551551</td>\n",
       "      <td>130.590776</td>\n",
       "      <td>134.287500</td>\n",
       "      <td>29619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15</th>\n",
       "      <td>133.360809</td>\n",
       "      <td>134.347270</td>\n",
       "      <td>132.155150</td>\n",
       "      <td>132.444105</td>\n",
       "      <td>58569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-18</th>\n",
       "      <td>136.698822</td>\n",
       "      <td>137.884564</td>\n",
       "      <td>133.291068</td>\n",
       "      <td>133.380743</td>\n",
       "      <td>25699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-19</th>\n",
       "      <td>137.605576</td>\n",
       "      <td>138.273175</td>\n",
       "      <td>136.957894</td>\n",
       "      <td>137.505928</td>\n",
       "      <td>20661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-20</th>\n",
       "      <td>139.159988</td>\n",
       "      <td>142.565750</td>\n",
       "      <td>138.910883</td>\n",
       "      <td>139.827588</td>\n",
       "      <td>33507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-21</th>\n",
       "      <td>141.292328</td>\n",
       "      <td>141.521500</td>\n",
       "      <td>139.970083</td>\n",
       "      <td>140.266017</td>\n",
       "      <td>18101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>142.209030</td>\n",
       "      <td>142.737131</td>\n",
       "      <td>141.546402</td>\n",
       "      <td>141.621146</td>\n",
       "      <td>18494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>142.308670</td>\n",
       "      <td>143.429642</td>\n",
       "      <td>141.989808</td>\n",
       "      <td>142.468086</td>\n",
       "      <td>11170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>140.933624</td>\n",
       "      <td>142.806898</td>\n",
       "      <td>140.546009</td>\n",
       "      <td>142.318647</td>\n",
       "      <td>17288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>140.774185</td>\n",
       "      <td>141.760646</td>\n",
       "      <td>140.323808</td>\n",
       "      <td>141.342152</td>\n",
       "      <td>12192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>140.425430</td>\n",
       "      <td>140.928627</td>\n",
       "      <td>139.399119</td>\n",
       "      <td>140.176325</td>\n",
       "      <td>14872700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            Close        High         Low        Open    Volume\n",
       "Ticker            GOOG        GOOG        GOOG        GOOG      GOOG\n",
       "Date                                                                \n",
       "2023-12-11  134.217743  134.307417  132.414226  133.340904  24502900\n",
       "2023-12-12  133.161530  134.058301  132.354432  132.792859  26584000\n",
       "2023-12-13  133.490372  134.297469  132.474019  134.063310  25414500\n",
       "2023-12-14  132.723114  134.551551  130.590776  134.287500  29619100\n",
       "2023-12-15  133.360809  134.347270  132.155150  132.444105  58569400\n",
       "2023-12-18  136.698822  137.884564  133.291068  133.380743  25699800\n",
       "2023-12-19  137.605576  138.273175  136.957894  137.505928  20661000\n",
       "2023-12-20  139.159988  142.565750  138.910883  139.827588  33507300\n",
       "2023-12-21  141.292328  141.521500  139.970083  140.266017  18101500\n",
       "2023-12-22  142.209030  142.737131  141.546402  141.621146  18494700\n",
       "2023-12-26  142.308670  143.429642  141.989808  142.468086  11170100\n",
       "2023-12-27  140.933624  142.806898  140.546009  142.318647  17288400\n",
       "2023-12-28  140.774185  141.760646  140.323808  141.342152  12192500\n",
       "2023-12-29  140.425430  140.928627  139.399119  140.176325  14872700"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Нормализация данных (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data)\n",
    "val_data_normalized = scaler.transform(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c2f1342a-d0eb-429c-b9c9-1ca49ee752b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15592511, 0.02658273, 0.15996527, 0.08946534, 0.28128685],\n",
       "       [0.0457371 , 0.        , 0.15471978, 0.03479198, 0.32519257],\n",
       "       [0.08004311, 0.02552123, 0.16521077, 0.16153314, 0.30051921],\n",
       "       [0.        , 0.05263386, 0.        , 0.18389854, 0.38922516],\n",
       "       [0.06652669, 0.03083541, 0.13723742, 0.        , 1.        ],\n",
       "       [0.41476029, 0.40829404, 0.23688785, 0.09343968, 0.30653828],\n",
       "       [0.50935611, 0.44976207, 0.55856648, 0.5049713 , 0.20023291],\n",
       "       [0.6715181 , 0.90781557, 0.72989593, 0.73658191, 0.4712559 ],\n",
       "       [0.8939715 , 0.79638538, 0.82281606, 0.78031991, 0.14623423],\n",
       "       [0.9896052 , 0.92610334, 0.96110144, 0.91550867, 0.15452971],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.        ],\n",
       "       [0.85655023, 0.93354806, 0.87334021, 0.98509191, 0.12907997],\n",
       "       [0.83991697, 0.82190425, 0.85384717, 0.88767597, 0.02156994],\n",
       "       [0.80353359, 0.7331209 , 0.77272726, 0.77137223, 0.07811508]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40665bed-a2ed-4aa9-8a1c-55bfe93686b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Создание кастомного Dataset\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.targets = targets.astype(np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "# 5. Создание Dataset\n",
    "trainset = IrisDataset(train_data_normalized, train_target)\n",
    "valset = IrisDataset(val_data_normalized, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8d485-2fd5-4856-b2c6-724c4bf985ff",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7722f-bcf1-4d83-8165-15f24444d9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50b1b6c-ace4-4f82-a6d7-335f24451851",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08efd0d-c21c-4df8-83ac-7a60d9708435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Создание данных\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# 2. Определение модели (Autoencoder)\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)  # Сжимаем до 2-мерного представления\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded  # Возвращаем закодированное представление\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# 3. Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Обучение\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    encoded, decoded = autoencoder(X)\n",
    "    loss = criterion(decoded, X)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 5. Кластеризация (K-Means можно применить к закодированным данным,  пропущено)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcc64f-1239-453a-8832-08f035b8eba6",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b70c6-1781-4f1f-b391-3d54d14e4186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
